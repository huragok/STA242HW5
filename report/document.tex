%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% University/School Laboratory Report
% LaTeX Template
% Version 3.0 (4/2/13)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Linux and Unix Users Group at Virginia Tech Wiki
% (https://vtluug.org/wiki/Example_LaTeX_chem_lab_report)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{article}

\usepackage{mhchem} % Package for chemical equation typesetting
\usepackage{siunitx} % Provides the \SI{}{} command for typesetting SI units
\usepackage{hyperref}
\usepackage{graphicx} % Required for the inclusion of images
\usepackage{tabularx}
\usepackage{float}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{bm}
\usepackage{multirow}% http://ctan.org/pkg/multirow
\usepackage{hhline}% http://ctan.org/pkg/hhline
\usepackage{caption}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{xcolor}
\usepackage[letterpaper, margin=1in]{geometry}
\lstset{
    %numbers=left,
    stepnumber=1,    
    firstnumber=1,
    numberfirstline=true,
    basicstyle=\ttfamily,
    keywordstyle=\color{blue}\ttfamily,
    stringstyle=\color{red}\ttfamily,
    commentstyle=\color{green}\ttfamily,
    breaklines=true,
}


\setlength\parindent{0pt} % Removes all indentation from paragraphs

\renewcommand{\labelenumi}{\alph{enumi}.} % Make numbering in the enumerate
% environment by letter rather than number (e.g. section 6)

%\usepackage{times} % Uncomment to use the Times New Roman font

%----------------------------------------------------------------------------------------
%	DOCUMENT INFORMATION
%----------------------------------------------------------------------------------------

\title{UC Davis STA 242 2015 Spring Assignment 5~\cite{wu2015NYCTaxi}} %
% Title
\author{Wenhao \textsc{Wu}, 998587583} % Author name
\date{\today} % Date for the report

\begin{document}
\maketitle % Insert the title, author and date

% If you wish to include an abstract, uncomment the lines below

\section{Algorithm Design}

\subsection{Compute the Deciles}
In order to compute the deciles of the total fare less the tolls, denoted as
$f_{net}$, we count the occurence of each value of $f_{net}$. The benefits are:
\begin{itemize}
    \item There are much more records than the possible values of $f_{net}$ in
    the original data. Consequently, it \textbf{saves tremendous memory usage}
    by counting the occurence.
    \item This algorithm is highly \textbf{compatible with parallel processing}.
    We can keep multiple tables to count the occurence of each value of
    $f_{net}$ for different data files, update these tables fully in parallel, and
    then merge these tables to compute the deciles.
\end{itemize}
In both of our implementations, we build a table to count the occurence for
each pair of data files by updating it seqentially as we read in a new
piece/bulk of record(s), then combine the 12 tables to compute the deciles.

\subsection{Solve The Linear Regression}
Denote the trip time as $t$ and the surcharge as $f_s$, respectively. In the two
regression tasks, the responses are denoted as $\mathbf{y}$, a $n$-by-1 vector
of $f_{net}$ in all records. In the first regresssion tasks, the predictors are
denoted as $\mathbf{X}_1$, a $n$-by-2 matrix where the first column represents
the $t$ from all records and the second column is an all 1 vector. In the second
regresssion tasks, the predictors are denoted as $\mathbf{X}_2$, a $n$-by-3
matrix where the first and the second columns represent the $t$, and $f_s$ from
all records and the third column is an all 1 vector. Theoretically, the
coefficients of the linear model can be computed as
\begin{align}
    \bm{\beta}_i =
    (\mathbf{X}_i^H\mathbf{X}_i)^{-1}\mathbf{X}_i^H\mathbf{y},\;i=1,2.
    \label{eq:lm}
\end{align}
Apparently, the sufficient statistic for the linear regression tasks are
$\mathbf{X}_i^H\mathbf{X}_i$ and $\mathbf{X}_i^H\mathbf{y}$, $i=1,2$ which has
very low dimension. Moreover, these sufficient statistics can be updated
sequentially as we read in a new piece/bulk of record(s), and are again highly
compatible with parallel processing.

In both of our implementations,  we update $\mathbf{X}_i^H\mathbf{X}_i$ and
$\mathbf{X}_i^H\mathbf{y}$, $i=1,2$ sequentially for each pair of data files,
then combine the 12 set of statistics by summing them up and solve the linear
problem as in~(\ref{eq:lm}) to get the coefficients for the regression models.

\section{Data Inspection, Pre-Processing and Extraction}
Due to the limited hard drive space available on my workstation, I keep the
original .zip files without decompressing them. Firstly we check that the
``data'' and ``fare'' files match each other row by row in the 3 index
fields ``medallion'', `` hack\_license'' and `` pickup\_datetime''. To do so, we
primarily make use of a combination of shell commands \texttt{unzip},
\texttt{cut}, \texttt{diff} , IO redirection and pipe commands to compare the 3
fields in each pair of files. (See \textbf{checkmatch.sh} in the Appendices.) We
verified that the files indeed match in pairs.

During the inspection, we also notice that ``trip\_fare\_8.csv.zip'',
``trip\_data\_9.csv.zip'' and ``trip\_fare\_9.csv.zip'' contains duplicated .csv
files, whcih are removed manually.

In both of our implementations, we build a ``connection'' to read in the output
of shell pipe commands to extract the data. The shell command to extract
`` surcharge'', ``tolls\_amount'' and ``total\_amount'' from the ``fare'' files
is
\begin{lstlisting}[language=sh]
    unzip -cq ../data/trip_fare_n.csv.zip | cut -d , -f 7,10,11
\end{lstlisting}

According to the data file description~\cite{}, roughly 7.5\% of all
trips' ``trip\_time'' is wrong so we take a safe approach to extract
``pickup\_datetime'' and ``dropoff\_datetime'' from the ``data'' files. The
corresponding shell command is
\begin{lstlisting}[language=sh]
    unzip -cq ../data/trip_data_n.csv.zip | cut -d , -f 6,7
\end{lstlisting}

Later we take the differences between them as the actual trip time. Fortunately,
both these two fields have a very neat format as ``\%Y-\%m-\%d \%H:\%M:\%S''
which can be easily processed.

\section{Implementation in Python}
Our first implementation is based on Python3. The pararllel processing is
implemented with package ``multiprocessing'': we define a worker function
\texttt{analyze\_file()} to compute the count of occurence table and the
sufficient statistics for the two linear regression tasks for a single pair of
data/fair files. A total of 12 copies of this worker function are mapped to a
pool of multiple processes and run in parallel. The results are then combined,
from which the deciles are computed and the 2 linear regression problems are
solved. 

The worker function \texttt{analyze\_file()} has a coroutine
structure~\cite{}: it is mainly composed of a ``source'' function
\texttt{parse\_file()} which read in one line from a pair of data/fare files,
process it, and send the result to a ``sink'' function
\texttt{accumulate\_lines()}, which is in charge of updating the count of
occurence table for the total amount less the toll and the sufficient statistics
for the regressions.

In terms of data structure, the count of occurence table is updated as a python
\texttt{dict} object and later converted to a pandas \texttt{Series} object to
enable easy combination. The suffficient statistics are represented as numpy
\texttt{ndarray} objects.

\section{Implementation in R}
Our second implementation is based on R. Similar to our first implementation,
we define a worker function \texttt{analyzeFile()} to compute the count of
occurence table and the sufficient statistics for the two linear regression
tasks for a single pair of data/fair files, then use \texttt{parLapply()} to run
it on a ``cluster'' for different files. After the 12 pairs of fare/data files
are all processed. The results are then combined with function
\texttt{reduceListSummaryNYCTaxi()} where the deciles are computed and the 2
linear regression problems are solved.

In the worker function \texttt{analyzeFile()}, instead of reading in 1 line from
a pair of fare/data files at a time as in our first implementation, we use
function \texttt{read.csv()} to read in a bulk of records as a data frame, and
then update the count of occurence table and the sufficient statistics for
regression. The benefits are two-fold. Firstly, we can use R's function
\texttt{table()} to count the occurence for this bulk of record and then update
the overall count of occurence table implemented with package ``hash'', which
prove to be more efficient than updating the table directly one line at a time.
Secondly, this bulk-style update would result in more accuracy in computing the
sufficient statistics theoretically.

In order to further speed up function \texttt{analyzeFile()}, we implement the
function to update the sufficient statistics, \texttt{updateSuffStat()}, in C++.

\section{Results}

\subsection{Deciles and Regression Results}

\subsection{Running Time Comparison}

\section{Conclusion}


%   BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

\bibliographystyle{unsrt}
\bibliography{myrefs}

%\pagebreak


\pagebreak
\section*{Appendix: Source Files}
%\subsection*{\texttt{BMLGrid.R}}
%\lstinputlisting[language=R]{../BMLGrid/R/BMLGrid.R}
%\subsection*{\texttt{BMLGrid.h}}
%\lstinputlisting[language=C++]{../BMLGrid/src/BMLGrid.h}
%\subsection*{\texttt{BMLGrid.cpp}}
%\lstinputlisting[language=C++]{../BMLGrid/src/BMLGrid.cpp}


%----------------------------------------------------------------------------------------


\end{document}