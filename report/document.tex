%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% University/School Laboratory Report
% LaTeX Template
% Version 3.0 (4/2/13)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Linux and Unix Users Group at Virginia Tech Wiki
% (https://vtluug.org/wiki/Example_LaTeX_chem_lab_report)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{article}

\usepackage{mhchem} % Package for chemical equation typesetting
\usepackage{siunitx} % Provides the \SI{}{} command for typesetting SI units
\usepackage{hyperref}
\usepackage{graphicx} % Required for the inclusion of images
\usepackage{tabularx}
\usepackage{float}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{bm}
\usepackage{multirow}% http://ctan.org/pkg/multirow
\usepackage{hhline}% http://ctan.org/pkg/hhline
\usepackage{caption}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{xcolor}
\usepackage[letterpaper, margin=0.9in]{geometry}
\lstset{
    %numbers=left,
    stepnumber=1,    
    firstnumber=1,
    numberfirstline=true,
    basicstyle=\ttfamily,
    keywordstyle=\color{blue}\ttfamily,
    stringstyle=\color{red}\ttfamily,
    commentstyle=\color{green}\ttfamily,
    breaklines=true,
}


\setlength\parindent{0pt} % Removes all indentation from paragraphs

\renewcommand{\labelenumi}{\alph{enumi}.} % Make numbering in the enumerate
% environment by letter rather than number (e.g. section 6)

%\usepackage{times} % Uncomment to use the Times New Roman font

%----------------------------------------------------------------------------------------
%	DOCUMENT INFORMATION
%----------------------------------------------------------------------------------------

\title{UC Davis STA 242 2015 Spring Assignment 5~\cite{wu2015NYCTaxi}} %
% Title
\author{Wenhao \textsc{Wu}, 998587583} % Author name
\date{\today} % Date for the report

\begin{document}
\maketitle % Insert the title, author and date

% If you wish to include an abstract, uncomment the lines below

\section{Algorithm Design}

\subsection{Compute the Deciles}
In order to compute the deciles of the total fare less the tolls, denoted as
$f_{net}$, we count the occurence of each value of $f_{net}$. The benefits are:
\begin{itemize}
    \item There are much more records than the possible values of $f_{net}$ in
    the original data. Consequently, it \textbf{saves tremendous memory usage}
    by counting the occurence.
    \item This algorithm is highly \textbf{compatible with parallel processing}.
    We can keep multiple tables to count the occurence of each value of
    $f_{net}$ for different data files, update these tables fully in parallel, and
    then merge these tables to compute the deciles.
\end{itemize}
In both of our implementations, we build a table to count the occurence for
each pair of data files by updating it seqentially as we read in a new
piece/bulk of record(s), then combine the 12 tables to compute the deciles.

\subsection{Solve The Linear Regression}
Denote the trip time as $t$ and the surcharge as $f_s$, respectively. In the two
regression tasks, the responses are denoted as $\mathbf{y}$, a $n$-by-1 vector
of $f_{net}$ in all records. In the first regresssion tasks, the predictors are
denoted as $\mathbf{X}_1$, a $n$-by-2 matrix where the first column represents
the $t$ from all records and the second column is an all 1 vector. In the second
regresssion tasks, the predictors are denoted as $\mathbf{X}_2$, a $n$-by-3
matrix where the first and the second columns represent the $t$, and $f_s$ from
all records and the third column is an all 1 vector. Theoretically, the
coefficients of the linear model can be computed as~\cite{hastie2009elements}
\begin{align}
    \bm{\beta}_i =
    (\mathbf{X}_i^H\mathbf{X}_i)^{-1}\mathbf{X}_i^H\mathbf{y},\;i=1,2.
    \label{eq:lm}
\end{align}
Apparently, the sufficient statistic for the linear regression tasks are
$\mathbf{X}_i^H\mathbf{X}_i$ and $\mathbf{X}_i^H\mathbf{y}$, $i=1,2$ which has
very low dimension. Moreover, these sufficient statistics can be updated
sequentially as we read in a new piece/bulk of record(s), and are again highly
compatible with parallel processing.

In both of our implementations,  we update $\mathbf{X}_i^H\mathbf{X}_i$ and
$\mathbf{X}_i^H\mathbf{y}$, $i=1,2$ sequentially for each pair of data files,
then combine the 12 set of statistics by summing them up and solve the linear
problem as in~(\ref{eq:lm}) to get the coefficients for the regression models.

\section{Data Inspection, Pre-Processing and Extraction}
Due to the limited hard drive space available on my workstation, I keep the
original .zip files without decompressing them. Firstly we check that the
``data'' and ``fare'' files match each other row by row in the 3 index
fields ``medallion'', `` hack\_license'' and `` pickup\_datetime''. To do so, we
primarily make use of a combination of shell commands \texttt{unzip},
\texttt{cut}, \texttt{diff} , IO redirection and pipe commands to compare the 3
fields in each pair of files. (See \textbf{checkmatch.sh} in the Appendices.) We
verified that the files indeed match in pairs.

During the inspection, we also notice that ``trip\_fare\_8.csv.zip'',
``trip\_data\_9.csv.zip'' and ``trip\_fare\_9.csv.zip'' contains duplicated .csv
files, whcih are removed manually.

In both of our implementations, we build a ``connection'' to read in the output
of shell pipe commands to extract the data. The shell command to extract
`` surcharge'', ``tolls\_amount'' and ``total\_amount'' from the ``fare'' files
is
\begin{lstlisting}[language=sh]
    unzip -cq ../data/trip_fare_n.csv.zip | cut -d , -f 7,10,11
\end{lstlisting}

According to the data file description~\cite{work2014new}, roughly 7.5\% of all
trips' ``trip\_time'' is wrong so we take a safe approach to extract
``pickup\_datetime'' and ``dropoff\_datetime'' from the ``data'' files. The
corresponding shell command is
\begin{lstlisting}[language=sh]
    unzip -cq ../data/trip_data_n.csv.zip | cut -d , -f 6,7
\end{lstlisting}

Later we take the differences between them as the actual trip time. Fortunately,
both these two fields have a very neat format as ``\%Y-\%m-\%d \%H:\%M:\%S''
which can be easily processed.

\section{Implementation in Python}
Our first implementation is based on Python3. The pararllel processing is
implemented with package ``multiprocessing'': we define a worker function
\texttt{analyze\_file()} to compute the count of occurence table and the
sufficient statistics for the two linear regression tasks for a single pair of
data/fair files. A total of 12 copies of this worker function are mapped to a
pool of multiple processes and run in parallel. The results are then combined,
from which the deciles are computed and the 2 linear regression problems are
solved. 

The worker function \texttt{analyze\_file()} has a coroutine
structure~\cite{beazley2009curious}: it is mainly composed of a ``source''
function \texttt{parse\_file()} which read in one line from a pair of data/fare files,
process it, and send the result to a ``sink'' function
\texttt{accumulate\_lines()}, which is in charge of updating the count of
occurence table for the total amount less the toll and the sufficient statistics
for the regressions.

In terms of data structure, the count of occurence table is updated as a python
\texttt{dict} object and later converted to a pandas \texttt{Series} object to
enable easy combination. The suffficient statistics are represented as numpy
\texttt{ndarray} objects.

\section{Implementation in R}
Our second implementation is based on R. Similar to our first implementation,
we define a worker function \texttt{analyzeFile()} to compute the count of
occurence table and the sufficient statistics for the two linear regression
tasks for a single pair of data/fair files, then use \texttt{parLapply()} to run
it on a ``cluster'' for different files. After the 12 pairs of fare/data files
are all processed. The results are then combined with function
\texttt{reduceListSummaryNYCTaxi()} where the deciles are computed and the 2
linear regression problems are solved.

In the worker function \texttt{analyzeFile()}, instead of reading in 1 line from
a pair of fare/data files at a time as in our first implementation, we use
function \texttt{read.csv()} to read in a bulk of 500000 records as a data
frame, and then update the count of occurence table and the sufficient statistics for
regression. The benefits are two-fold. Firstly, we can use R's function
\texttt{table()} to count the occurence for this bulk of record and then update
the overall count of occurence table implemented with package ``hash'', which
prove to be more efficient than updating the table directly one line at a time.
Secondly, this bulk-style update would result in more accuracy in computing the
sufficient statistics theoretically.

In order to further speed up function \texttt{analyzeFile()}, we implement the
function to update the sufficient statistics, \texttt{updateSuffStat()}, in C++.

\section{Results}

\subsection{Deciles and Linear Regression Results}
The deciles and the two linear regression results computed using our Python and
R implementation are presented in Table~\ref{tab:decile} and Table~\ref{tab:lm},
respectively. As we can see, the results of the two implementaions are very
close to each other. The difference is likely due to numerical accuracy issues.
We believe that the R implementation is more precise.

\begin{table}[!t]
    \renewcommand{\arraystretch}{1.3}
    \caption{Deciles of the total fare less the tolls. Implementation 1, 2 are
    based on python and R, respectively.}
    \label{tab:decile}
    \centering
    \begin{tabular}{c|ccccccccccc}
        \hline
         & 0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.9 & 1 \\
         \hline
        1 & -1430.0 & 6.0 & 7.5 & 8.5 & 9.75 & 11.0 & 13.0 & 15.0 & 18.5 &
        26.120000000000001 & 685908.09999999998 \\
        2 & -1430.00 & 6.00 & 7.50 & 8.50 & 9.75 & 11.00 & 13.00 & 15.00 & 18.50
        & 26.12 & 685908.10 \\
        \hline
    \end{tabular}
\end{table}

\begin{table}[!t]
    \renewcommand{\arraystretch}{1.3}
    \caption{Linear regression results}
    \label{tab:lm}
    \centering
    \begin{tabular}{c|cc|ccc}
        \hline
        \multirow{2}{*}{} & \multicolumn{2}{c|}{Linear model 1} &
        \multicolumn{3}{c}{Linear model 2} \\
        \hhline{~-----} & trip time & intercept & trip time & surcharge &
        intercept \\
        \hline
        Python & 2.02064511e-03   & 1.30134743e+01 &  2.02196225e-03 &
        3.04104587e-01 & 1.29153642e+01 \\
        R & 0.00202051 & 13.01345450 & 0.002021825 & 0.303964991 & 12.915390308
        \\
        \hline
    \end{tabular}
\end{table}

\subsection{Running Time Comparison}
We test the running time of both of our implementations using different number
of processes. In there tests, we use a Dell Precision T1700 workstation equipped
with 16GB DDR3 RAM, a Core i7-4790K CPU and a Samsung 850 PRO SSD in Ubuntu
14.04 OS. The elapsed time are plotted in Fig.~\ref{fig:running_time}. Without
parallel processing, the Python and R implementaions take 3861 and 1231 seconds
to go through all the 12 pairs of files. With a pool/cluster of size 12, the
elased times for both implementations reduce to 1144 and 344 seconds,
corresponding to a maximum speed-up of 3.58 and 3.34, respectively. We note
that, however, with a pool/cluster of size 6, we can achieve approximately the
same speed-up.

\begin{figure}[h]
    \centering
    \includegraphics[width=3.5in]{figs/runningTime.pdf}
    \caption{The elased time of both implementations vs different sizes of the
    pool/cluster $N$. The ideal times are computed as $t_{\mbox{ideal}}(N) =
    t_1/12 \times \lceil12 / N\rceil$, where $t_1$ is the elapsed time when
    $N = 1$. This equation is based on the assumption that there is no I/O
    bottleneck, all files take equal amount of time to process, and the time to
    setup the pool/cluster and combine results returned by multiple worker
    functions is negligible.}
    \label{fig:running_time}
\end{figure}

\section{Comments}
Here we make a brief comparison between the two implementations:
\begin{itemize}
    \item In terms of performance, our second implementation based on R clearly
    wins against the first implementation based on Python, although both
    implementations result in a decent short running time considering we are
    only using a small form factor workstation.
    \item In terms of programming time, it turns out that our second
    implementation takes longer than the first implementation as shown in the
    commit history. In fact, I start working on implementation 2 first and then
    implmentation 1. Consequently, the programming time difference is mainly due
    to the fact that we ecountered some problem playing with the original data
    when firstly working with implementation 2 and the author is not familiar
    with R. Since both implementations are based on a similar, highly specific
    algorithm design, it is expected that in capable hands the programming time
    would not be so different.
    \item Despite that our first implementation takes much longer time, it uses
    very little amount of memory. Should we reduce the bulk size in the second
    implementation to reduce the memory usage to the amoung of the first
    implmentation, it is going to run even more slowly. Consequently, if we are
    on a highly memory-limited platform (which is rarely the case), the first
    implementation is more appropriate. Otherwise the second implementation will
    be more appropriate due to its advantage in speed.
\end{itemize}


%   BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

\bibliographystyle{unsrt}
\bibliography{myrefs}

%\pagebreak


\pagebreak
\section*{Appendix: Source Files}
\subsection*{Verify row-by-row matching between
fare/data files: \textbf{checkmatch.sh}}
\lstinputlisting[language=sh]{../data/checkmatch.sh}
\subsection*{Implementation 1: \textbf{implementation1.py}}
\lstinputlisting[language=Python]{../implementation_1/implementation1.py}
\subsection*{Implementation 2: \textbf{implementation2.R}}
\lstinputlisting[language=R]{../implementation_2/implementation2.R}
\subsection*{Implementation 2: \textbf{NYCTaxi.R}}
\lstinputlisting[language=R]{../implementation_2/NYCTaxi/R/NYCTaxi.R}
\subsection*{Implementation 2: \textbf{NYCTaxi.h}}
\lstinputlisting[language=C++]{../implementation_2/NYCTaxi/src/NYCTaxi.h}
\subsection*{Implementation 2: \textbf{NYCTaxi.cpp}}
\lstinputlisting[language=C++]{../implementation_2/NYCTaxi/src/NYCTaxi.cpp}


%----------------------------------------------------------------------------------------


\end{document}